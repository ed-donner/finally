---
phase: 05-llm-chat-integration
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - backend/app/llm/router.py
  - backend/app/llm/__init__.py
  - backend/app/main.py
  - backend/tests/llm/test_chat_routes.py
autonomous: true

must_haves:
  truths:
    - "POST /api/chat with a user message returns 200 with AI response containing message, trades, and watchlist_changes"
    - "POST /api/chat with 'buy' keyword in mock mode returns executed trade result with correct fields"
    - "POST /api/chat with 'sell' keyword and no position returns failed trade result in response (not HTTP error)"
    - "POST /api/chat with 'add' keyword adds ticker to watchlist and returns applied result"
    - "POST /api/chat with empty or missing message returns 422 validation error"
    - "Chat router is mounted in main.py lifespan and accessible at /api/chat"
    - "Full app integration: health check + chat endpoint both work in the assembled app"
  artifacts:
    - path: "backend/app/llm/router.py"
      provides: "create_chat_router factory returning APIRouter with POST /api/chat"
    - path: "backend/app/main.py"
      provides: "Updated lifespan that includes chat router"
      contains: "create_chat_router"
    - path: "backend/tests/llm/test_chat_routes.py"
      provides: "HTTP-level tests for POST /api/chat endpoint"
  key_links:
    - from: "backend/app/llm/router.py"
      to: "app.llm.service.process_chat_message"
      via: "direct call in endpoint handler"
      pattern: "process_chat_message\\("
    - from: "backend/app/main.py"
      to: "app.llm.router.create_chat_router"
      via: "include_router in lifespan"
      pattern: "create_chat_router\\(db, price_cache, market_source\\)"
---

<objective>
Create the chat router factory, wire it into the FastAPI application lifespan, and write HTTP-level endpoint tests. This exposes the LLM service (built in Plan 01) as POST /api/chat.

Purpose: Complete the API surface for the chat feature. After this plan, POST /api/chat is a working endpoint that accepts user messages and returns AI responses with auto-executed actions.

Output: Working POST /api/chat endpoint accessible in the assembled app, with passing route tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-llm-chat-integration/05-RESEARCH.md
@.planning/phases/05-llm-chat-integration/05-01-SUMMARY.md

@backend/app/main.py
@backend/app/llm/service.py
@backend/app/llm/models.py
@backend/app/routes/portfolio.py
@backend/app/watchlist/router.py
@backend/tests/routes/test_portfolio_routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat router and wire into main.py</name>
  <files>
    backend/app/llm/router.py
    backend/app/llm/__init__.py
    backend/app/main.py
  </files>
  <action>
**backend/app/llm/router.py** — Router factory following the exact pattern from create_portfolio_router and create_watchlist_router:
- `create_chat_router(db: aiosqlite.Connection, price_cache: PriceCache, market_source: MarketDataSource) -> APIRouter`
- Creates `APIRouter(prefix="/api", tags=["chat"])`
- Single endpoint: `@router.post("/chat", response_model=ChatResponse)`
- Endpoint body: calls `process_chat_message(db, price_cache, market_source, request.message)` and returns the result
- Import ChatRequest, ChatResponse from app.llm.models
- Import process_chat_message from app.llm.service
- No try/except — process_chat_message handles all error collection internally and always returns a ChatResponse

**backend/app/llm/__init__.py** — Update exports to include router:
- Add `create_chat_router` from `.router`
- Keep existing exports: `process_chat_message`, `ChatRequest`, `ChatResponse`

**backend/app/main.py** — Add chat router to lifespan:
- Add import: `from app.llm import create_chat_router`
- In the lifespan function, after the existing router mounts (after `create_watchlist_router`), add:
  `app.include_router(create_chat_router(db, price_cache, market_source))`
- Keep the static files mount LAST (it must remain after all API routers)
- No other changes to main.py
  </action>
  <verify>
```bash
cd /Users/ed/projects/finally/backend && uv run python -c "from app.llm import create_chat_router; print('Router import OK')"
cd /Users/ed/projects/finally/backend && uv run python -c "from app.main import app; routes = [r.path for r in app.routes]; print('Routes:', routes)"
```
Router imports without error. The app object exists (routes are registered in lifespan, not at import time, so we just verify no import errors).
  </verify>
  <done>
Router factory created at backend/app/llm/router.py. main.py lifespan includes chat router. POST /api/chat endpoint will be mounted when the app starts, serving the full chat processing pipeline.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write HTTP-level tests for POST /api/chat</name>
  <files>
    backend/tests/llm/test_chat_routes.py
  </files>
  <action>
**backend/tests/llm/test_chat_routes.py** — Following the exact pattern from tests/routes/test_portfolio_routes.py:

Fixtures:
- `app` fixture: creates isolated DB via init_db(tmp_path), PriceCache with AAPL=150.00 and GOOGL=175.00, a mock MarketDataSource (same minimal mock as in Plan 01 conftest — or import it if it's factored into conftest). Creates FastAPI(), includes chat router via create_chat_router(db, price_cache, mock_market_source). Sets LLM_MOCK=true via monkeypatch. Yields app. Closes DB.
- `client` fixture: ASGITransport + AsyncClient with base_url="http://test"

Tests (all use LLM_MOCK=true):
- `test_post_chat_default_message`: POST /api/chat with {"message": "hello"} -> 200, response has "message" string (non-empty), trades=[], watchlist_changes=[]
- `test_post_chat_buy_executes_trade`: POST /api/chat with {"message": "buy some AAPL"} -> 200, response has message, trades list with one entry having status="executed", ticker="AAPL", side="buy"
- `test_post_chat_buy_updates_portfolio`: POST /api/chat with "buy" message, then GET portfolio state from DB to verify cash decreased (requires also mounting portfolio router, or just check DB directly)
- `test_post_chat_sell_failure_reported`: POST /api/chat with {"message": "sell AAPL"} (no position) -> 200 (NOT 400), response trades has one entry with status="failed", error containing "Insufficient"
- `test_post_chat_watchlist_add`: POST /api/chat with {"message": "add to watchlist"} -> 200, watchlist_changes has one entry with status="applied", ticker="PYPL", action="add"
- `test_post_chat_empty_message_rejected`: POST /api/chat with {"message": ""} -> 422
- `test_post_chat_missing_message_rejected`: POST /api/chat with {} -> 422
- `test_post_chat_messages_persisted`: POST /api/chat, then query chat_messages table directly -> 2 rows (user + assistant)

Use monkeypatch.setenv("LLM_MOCK", "true") in the app fixture to ensure mock mode for all tests.
  </action>
  <verify>
```bash
cd /Users/ed/projects/finally/backend && uv run --extra dev pytest tests/llm/test_chat_routes.py -v
```
All route tests pass.
```bash
cd /Users/ed/projects/finally/backend && uv run --extra dev pytest -v
```
Full test suite passes (all existing + all new LLM tests).
```bash
cd /Users/ed/projects/finally/backend && uv run --extra dev ruff check app/ tests/
```
No lint errors.
  </verify>
  <done>
POST /api/chat endpoint is fully tested at the HTTP level. Default messages return AI responses. Buy messages trigger trade execution. Sell without position reports failure (200, not 400). Watchlist changes apply. Validation rejects empty/missing messages. Messages persist in database. Full backend test suite passes.
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/ed/projects/finally/backend && uv run --extra dev pytest tests/llm/ -v` — all LLM tests pass (service + route)
2. `cd /Users/ed/projects/finally/backend && uv run --extra dev pytest -v` — full suite passes
3. `cd /Users/ed/projects/finally/backend && uv run --extra dev ruff check app/ tests/` — no lint errors
4. Verify main.py includes chat router in lifespan
5. Verify POST /api/chat returns 200 with structured response in mock mode
</verification>

<success_criteria>
1. POST /api/chat with a user message returns 200 with ChatResponse (message + trades + watchlist_changes)
2. Buy message in mock mode executes trade, cash decreases, trade result shows status=executed
3. Sell without position returns 200 with status=failed in trades (NOT an HTTP error)
4. Watchlist add applies and returns status=applied
5. Empty/missing message returns 422
6. Chat router is mounted in main.py lifespan alongside portfolio, watchlist, and stream routers
7. All backend tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-chat-integration/05-02-SUMMARY.md`
</output>
