---
phase: 05-llm-chat-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/pyproject.toml
  - backend/app/llm/__init__.py
  - backend/app/llm/models.py
  - backend/app/llm/prompt.py
  - backend/app/llm/mock.py
  - backend/app/llm/service.py
  - backend/tests/llm/__init__.py
  - backend/tests/llm/conftest.py
  - backend/tests/llm/test_service.py
autonomous: true

must_haves:
  truths:
    - "Chat service builds a prompt containing the user's actual cash balance, positions with P&L, and watchlist prices"
    - "Chat service calls LiteLLM with extra_body for structured output and Cerebras provider routing"
    - "Chat service parses LLM JSON response into validated Pydantic model with fallback for malformed output"
    - "Trades in the LLM response auto-execute through execute_trade and results are collected"
    - "Failed trades (insufficient cash/shares) appear as error entries, not exceptions"
    - "Watchlist changes in the LLM response add/remove tickers through existing service functions"
    - "Failed watchlist changes (duplicate 409, missing 404) appear as error entries, not exceptions"
    - "Both user and assistant messages persist in chat_messages table with executed actions"
    - "Recent conversation history (last 20 messages) is loaded and included in LLM context"
    - "When LLM_MOCK=true, deterministic responses are returned without calling the LLM API"
    - "Portfolio snapshot is recorded after any successful trade execution"
  artifacts:
    - path: "backend/app/llm/models.py"
      provides: "Pydantic schemas: ChatRequest, ChatResponse, ChatLLMResponse, TradeAction, WatchlistAction, TradeResult, WatchlistResult"
    - path: "backend/app/llm/prompt.py"
      provides: "build_system_prompt function that formats portfolio + watchlist into system message"
    - path: "backend/app/llm/mock.py"
      provides: "get_mock_response function with keyword-based deterministic responses"
    - path: "backend/app/llm/service.py"
      provides: "process_chat_message orchestrator, save_chat_message, load_chat_history, parse_llm_response"
    - path: "backend/tests/llm/test_service.py"
      provides: "Unit tests covering mock mode, action execution, error collection, persistence, history"
  key_links:
    - from: "backend/app/llm/service.py"
      to: "app.portfolio.service.execute_trade"
      via: "direct import and call"
      pattern: "execute_trade\\(db, price_cache"
    - from: "backend/app/llm/service.py"
      to: "app.watchlist.service.add_ticker / remove_ticker"
      via: "direct import and call"
      pattern: "(add_ticker|remove_ticker)\\(db,"
    - from: "backend/app/llm/service.py"
      to: "app.portfolio.snapshots.record_snapshot"
      via: "called after successful trades"
      pattern: "record_snapshot\\(db, price_cache\\)"
    - from: "backend/app/llm/service.py"
      to: "litellm.acompletion"
      via: "async LLM call with extra_body"
      pattern: "acompletion.*extra_body"
---

<objective>
Create the complete LLM chat service layer: Pydantic models, system prompt builder, mock mode, and the core process_chat_message orchestrator that builds context, calls the LLM (or mock), parses structured output, auto-executes trades and watchlist changes through existing services, persists messages, and returns results. Plus comprehensive unit tests.

Purpose: This is the business logic core of the AI chat feature. It wires together all existing backend services (portfolio, watchlist, price cache) into a single chat processing pipeline that the router (Plan 02) will expose as POST /api/chat.

Output: Working `app/llm/` module with models, prompt, mock, service, and passing unit tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-llm-chat-integration/05-RESEARCH.md

@backend/app/portfolio/service.py
@backend/app/portfolio/snapshots.py
@backend/app/watchlist/service.py
@backend/app/market/cache.py
@backend/app/market/interface.py
@backend/app/db/schema.py
@backend/tests/portfolio/conftest.py
@backend/tests/portfolio/test_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM module — models, prompt, mock, service</name>
  <files>
    backend/pyproject.toml
    backend/app/llm/__init__.py
    backend/app/llm/models.py
    backend/app/llm/prompt.py
    backend/app/llm/mock.py
    backend/app/llm/service.py
  </files>
  <action>
First, add litellm to backend dependencies:
```bash
cd backend && uv add litellm
```

**backend/app/llm/models.py** — Pydantic schemas:
- `TradeAction(BaseModel)`: ticker (str), side (str, pattern "^(buy|sell)$"), quantity (float, gt=0)
- `WatchlistAction(BaseModel)`: ticker (str), action (str, pattern "^(add|remove)$")
- `ChatLLMResponse(BaseModel)`: message (str), trades (list[TradeAction], default []), watchlist_changes (list[WatchlistAction], default [])
- `ChatRequest(BaseModel)`: message (str, min_length=1)
- `TradeResult(BaseModel)`: status (str — "executed" or "failed"), ticker (str), side (str), quantity (float | None = None), price (float | None = None), total (float | None = None), error (str | None = None)
- `WatchlistResult(BaseModel)`: status (str — "applied" or "failed"), ticker (str), action (str), error (str | None = None)
- `ChatResponse(BaseModel)`: message (str), trades (list[TradeResult], default []), watchlist_changes (list[WatchlistResult], default [])

**backend/app/llm/prompt.py** — System prompt builder:
- `build_system_prompt(portfolio: dict, watchlist_prices: list[dict]) -> str`
- Takes the dict from `get_portfolio()` (cash_balance, positions with all fields, total_value)
- Takes list of dicts with ticker + price from the watchlist
- Formats into a system prompt that:
  - Identifies as "FinAlly, an AI trading assistant for a simulated portfolio"
  - Instructs to be concise, data-driven, analyze positions, suggest and execute trades, manage watchlist
  - Includes full portfolio state: cash, each position (ticker, qty, avg cost, current price, P&L, P&L %), total value
  - Includes watchlist prices
  - Reinforces JSON response format with the exact schema (belt-and-suspenders with structured output)
  - Specifies trades and watchlist_changes are optional arrays (empty if no actions)

**backend/app/llm/mock.py** — Deterministic mock responses:
- `get_mock_response(user_message: str) -> str` returns JSON string
- Keyword matching (case-insensitive):
  - "buy" -> response with trades: [{"ticker": "AAPL", "side": "buy", "quantity": 5}]
  - "sell" -> response with trades: [{"ticker": "AAPL", "side": "sell", "quantity": 5}]
  - "add" or "watch" -> response with watchlist_changes: [{"ticker": "PYPL", "action": "add"}]
  - "remove" -> response with watchlist_changes: [{"ticker": "PYPL", "action": "remove"}]
  - default -> message-only response referencing portfolio awareness
- Each response has a contextual message string (e.g., "Done! I've bought 5 shares of AAPL for you.")

**backend/app/llm/service.py** — Core orchestrator:
- `parse_llm_response(content: str) -> ChatLLMResponse`: try model_validate_json, fallback to ChatLLMResponse(message=content, trades=[], watchlist_changes=[])
- `save_chat_message(db, role: str, content: str, actions: dict | None = None) -> None`: INSERT into chat_messages with uuid4 id, user_id="default", ISO timestamp, json.dumps(actions) if present
- `load_chat_history(db, limit: int = 20) -> list[dict]`: SELECT role, content from chat_messages ORDER BY created_at DESC LIMIT ?, reversed to chronological. Returns list of {"role": ..., "content": ...}
- `process_chat_message(db, price_cache, market_source, user_message: str) -> ChatResponse`:
  1. Get portfolio via `get_portfolio(db, price_cache)`
  2. Get watchlist via `get_watchlist(db)`, enrich with prices from `price_cache.get_price(ticker)`
  3. Load chat history via `load_chat_history(db)`
  4. Build messages array: system prompt + history + user message
  5. Check `os.environ.get("LLM_MOCK", "").lower() == "true"` — if so, use `get_mock_response(user_message)`, else call `litellm.acompletion` with:
     - model="openrouter/openai/gpt-oss-120b"
     - messages=messages
     - extra_body with response_format (json_schema type, strict True, schema from ChatLLMResponse.model_json_schema()) and provider (order: ["Cerebras"], allow_fallbacks: True)
  6. Parse response via `parse_llm_response(content)`
  7. Execute trades: loop through parsed.trades, try `execute_trade(db, price_cache, t.ticker, t.side, t.quantity)`, catch ValueError and Exception. Build TradeResult for each (executed with trade dict fields, or failed with error string)
  8. Execute watchlist changes: loop through parsed.watchlist_changes, try add_ticker/remove_ticker on db + market_source.add_ticker/remove_ticker. Catch HTTPException (get detail attr) and Exception. Build WatchlistResult for each
  9. If any trades executed successfully, call `record_snapshot(db, price_cache)`
  10. Save user message: `save_chat_message(db, "user", user_message)`
  11. Save assistant message: `save_chat_message(db, "assistant", parsed.message, {"trades": [r.model_dump() for r in trade_results], "watchlist_changes": [r.model_dump() for r in watchlist_results]})`
  12. Return `ChatResponse(message=parsed.message, trades=trade_results, watchlist_changes=watchlist_results)`

**backend/app/llm/__init__.py** — Public API exports:
- Export: `create_chat_router` (from .router — will be added in Plan 02), `process_chat_message`, `ChatRequest`, `ChatResponse`
- For now, only export service and models (router comes in Plan 02)

Note: Use `from __future__ import annotations` where needed. Follow existing style: short modules, clear docstrings, no defensive over-engineering.
  </action>
  <verify>
```bash
cd /Users/ed/projects/finally/backend && uv run python -c "from app.llm.models import ChatRequest, ChatResponse, ChatLLMResponse; print('Models OK')"
cd /Users/ed/projects/finally/backend && uv run python -c "from app.llm.prompt import build_system_prompt; print('Prompt OK')"
cd /Users/ed/projects/finally/backend && uv run python -c "from app.llm.mock import get_mock_response; print(get_mock_response('buy some AAPL'))"
cd /Users/ed/projects/finally/backend && uv run python -c "from app.llm.service import process_chat_message, parse_llm_response; print('Service OK')"
```
All four import checks pass without errors.
  </verify>
  <done>
All 5 files in backend/app/llm/ exist with correct implementations. Models define the full request/response chain. Prompt builder formats real portfolio data. Mock returns deterministic keyword-matched JSON. Service orchestrates the full flow: context -> LLM/mock -> parse -> execute actions -> persist -> respond. litellm is in pyproject.toml dependencies.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write unit tests for LLM service layer</name>
  <files>
    backend/tests/llm/__init__.py
    backend/tests/llm/conftest.py
    backend/tests/llm/test_service.py
  </files>
  <action>
**backend/tests/llm/__init__.py** — Empty file.

**backend/tests/llm/conftest.py** — Test fixtures following the portfolio test pattern:
- `db` fixture: create isolated DB via `init_db(tmp_path / "test.db")`, yield, close
- `price_cache` fixture: PriceCache with AAPL=150.00, GOOGL=175.00, MSFT=400.00, PYPL=80.00
- `market_source` fixture: a simple mock MarketDataSource. Create a minimal class that extends MarketDataSource, implements add_ticker/remove_ticker as no-ops (store calls in a list for assertions), start/stop as no-ops, get_tickers returns empty list. This avoids importing the actual simulator.

**backend/tests/llm/test_service.py** — Tests (all run with LLM_MOCK=true via monkeypatch):

*Parsing tests:*
- `test_parse_llm_response_valid_json`: valid JSON with message + trades + watchlist_changes parses correctly
- `test_parse_llm_response_message_only`: JSON with only "message" field, no trades/watchlist_changes -> defaults to empty lists
- `test_parse_llm_response_invalid_json_fallback`: non-JSON string -> fallback response with message=content, empty trades/watchlist_changes
- `test_parse_llm_response_malformed_json_fallback`: invalid JSON string -> same fallback

*Mock mode tests:*
- `test_mock_response_buy_keyword`: message containing "buy" returns response with buy trade action
- `test_mock_response_sell_keyword`: message containing "sell" returns response with sell trade action
- `test_mock_response_watchlist_keyword`: message containing "add" returns watchlist change
- `test_mock_response_default`: generic message returns portfolio-aware message, no actions

*Chat history tests:*
- `test_save_and_load_chat_history`: save 3 messages, load with limit=20, verify chronological order and content
- `test_load_chat_history_limit`: save 5 messages, load with limit=2, verify only latest 2 returned
- `test_load_chat_history_empty`: load from empty DB, returns empty list

*Full process_chat_message tests (all with monkeypatch setting LLM_MOCK=true):*
- `test_process_chat_default_message`: send generic message -> response has message, empty trades/watchlist_changes
- `test_process_chat_buy_executes_trade`: send "buy" message -> response has message + executed trade result with ticker=AAPL, side=buy, quantity=5, status=executed. Verify cash decreased in DB.
- `test_process_chat_sell_without_position_reports_failure`: send "sell" message (no position) -> trade result has status=failed with error containing "Insufficient shares"
- `test_process_chat_watchlist_add`: send "add to watchlist" message -> watchlist_changes result has status=applied, ticker=PYPL, action=add. Verify PYPL in watchlist table. Verify market_source.add_ticker was called.
- `test_process_chat_messages_persisted`: send a message -> check chat_messages table has 2 rows (user + assistant), correct roles and content
- `test_process_chat_history_included`: save a prior user message in DB, then call process_chat_message -> verify the function doesn't crash (history is loaded and formatted into messages)
- `test_process_chat_snapshot_after_trade`: send "buy" message -> verify portfolio_snapshots table has a new row (snapshot recorded after trade)

Use `monkeypatch.setenv("LLM_MOCK", "true")` in each test or via a fixture. Use the conftest db, price_cache, and market_source fixtures. All tests should be async.
  </action>
  <verify>
```bash
cd /Users/ed/projects/finally/backend && uv run --extra dev pytest tests/llm/ -v
```
All tests pass.
```bash
cd /Users/ed/projects/finally/backend && uv run --extra dev pytest -v
```
Full test suite passes (145 existing + new LLM tests).
  </verify>
  <done>
All LLM service tests pass. Tests cover: response parsing (valid, message-only, fallback), mock mode (keyword matching), chat history (save/load/limit/empty), and full process_chat_message flow (default message, buy execution, sell failure reporting, watchlist add, message persistence, history inclusion, snapshot after trade). Full backend test suite still passes.
  </done>
</task>

</tasks>

<verification>
1. `cd /Users/ed/projects/finally/backend && uv run --extra dev pytest tests/llm/ -v` — all LLM tests pass
2. `cd /Users/ed/projects/finally/backend && uv run --extra dev pytest -v` — full suite passes (145 existing + new)
3. `cd /Users/ed/projects/finally/backend && uv run --extra dev ruff check app/llm/ tests/llm/` — no lint errors
4. Verify litellm in pyproject.toml dependencies
</verification>

<success_criteria>
1. backend/app/llm/ contains 5 files: __init__.py, models.py, prompt.py, mock.py, service.py
2. All Pydantic models validate correctly (ChatRequest, ChatResponse, ChatLLMResponse with nested TradeAction, WatchlistAction)
3. Mock mode returns deterministic keyword-matched responses without touching LLM API
4. process_chat_message with LLM_MOCK=true: sends default message -> gets response; sends "buy" -> trade executes and cash decreases; sends "sell" without position -> failure reported; sends "add" -> watchlist updated
5. Chat messages persist in DB with correct roles and actions JSON
6. All backend tests pass (existing 145 + new LLM tests)
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-chat-integration/05-01-SUMMARY.md`
</output>
